Name,Objective,Variation,Evaluation_Metric,Model,Accuracy,Precision,Recall,F1_Score,AUC_ROC,Cross_Val_Mean,Cross_Val_Std,Train_Score,Test_Score
DT-1: Baseline,Establish baseline performance with default Decision Tree parameters,"Default parameters (no max_depth limit, min_samples_split=2)","Accuracy, F1-Score",DecisionTreeClassifier,0.9122807017543859,0.9160721496836722,0.9122807017543859,0.9130211893369787,0.9156746031746031,0.9098901098901099,0.01890620937811566,1.0,0.9122807017543859
DT-2: Pruned + Feature Selection,Reduce overfitting through pruning and feature selection to improve generalization,"max_depth=5, min_samples_split=20, top 15 features selected","Accuracy, F1-Score",DecisionTreeClassifier,0.9210526315789473,0.9207976506585542,0.9210526315789473,0.9208492773270941,0.9474206349206349,0.9296703296703296,0.02915714101411341,0.9714285714285714,0.9210526315789473
RF-1: Small Forest + Normalization,Test impact of data normalization on ensemble performance with small forest,"n_estimators=50, normalized features using MinMaxScaler","Accuracy, AUC-ROC",RandomForestClassifier,0.956140350877193,0.9560729421281235,0.956140350877193,0.9560273762928302,0.9917328042328043,0.9516483516483518,0.02563055777953977,1.0,0.956140350877193
RF-2: Grid Search Optimized,Find optimal hyperparameters to maximize model performance,"Grid search: n_estimators=[100,200], max_depth=[10,20,None], min_samples_split=[2,5]",F1-Score (optimization target),RandomForestClassifier,0.9473684210526315,0.9473684210526315,0.9473684210526315,0.9473684210526315,0.9933862433862434,0.9538461538461538,0.022413272587221002,0.9956043956043956,0.9473684210526315
AB-1: Weak Learners,Test boosting with intentionally weak base estimators,"n_estimators=30, max_depth=1 for base estimator (decision stumps)","Accuracy, Precision",AdaBoostClassifier,0.9473684210526315,0.9474395448079658,0.9473684210526315,0.947087062795646,0.9801587301587301,0.9648351648351647,0.02346610604842044,1.0,0.9473684210526315
AB-2: Complex + Different Split,Increase model complexity and test generalization with different data split,"n_estimators=100, learning_rate=0.5, 70-30 train-test split","Recall, AUC-ROC",AdaBoostClassifier,0.9532163742690059,0.9537010694905431,0.9532163742690059,0.9528910072359551,0.9903621495327103,0.9649367088607596,0.029952594334825798,1.0,0.9532163742690059
